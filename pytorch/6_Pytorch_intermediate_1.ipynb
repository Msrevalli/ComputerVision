{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0842,  0.0748,  1.4731,  1.4644],\n",
      "        [ 1.4561,  1.0404,  0.4604,  0.6550],\n",
      "        [ 0.3911, -2.2030,  0.0385, -1.3155],\n",
      "        [ 0.4131,  2.5279, -0.2601,  0.5025]])\n",
      "torch.Size([4, 4])\n",
      "--------------------\n",
      "tensor([-0.0842,  0.0748,  1.4731,  1.4644,  1.4561,  1.0404,  0.4604,  0.6550,\n",
      "         0.3911, -2.2030,  0.0385, -1.3155,  0.4131,  2.5279, -0.2601,  0.5025])\n",
      "torch.Size([16])\n",
      "--------------------\n",
      "shape '[-1, 7]' is invalid for input of size 16\n",
      "tensor([[-0.0842,  0.0748,  1.4731,  1.4644,  1.4561,  1.0404,  0.4604,  0.6550],\n",
      "        [ 0.3911, -2.2030,  0.0385, -1.3155,  0.4131,  2.5279, -0.2601,  0.5025]])\n",
      "torch.Size([2, 8])\n",
      "--------------------\n",
      "tensor([[  -8.4233,    7.4820,  147.3057,  146.4385],\n",
      "        [ 145.6107,  104.0439,   46.0391,   65.4980],\n",
      "        [  39.1137, -220.3028,    3.8524, -131.5459],\n",
      "        [  41.3064,  252.7894,  -26.0100,   50.2508]])\n",
      "tensor([-0.0842,  0.0748,  1.4731,  1.4644,  1.4561,  1.0404,  0.4604,  0.6550,\n",
      "         0.3911, -2.2030,  0.0385, -1.3155,  0.4131,  2.5279, -0.2601,  0.5025])\n",
      "tensor([[-0.0842,  0.0748,  1.4731,  1.4644,  1.4561,  1.0404,  0.4604,  0.6550],\n",
      "        [ 0.3911, -2.2030,  0.0385, -1.3155,  0.4131,  2.5279, -0.2601,  0.5025]])\n"
     ]
    }
   ],
   "source": [
    "# View\n",
    "# Returns a new tensor with the same data but of a different shape.\n",
    "# If you're also concerned about memory usage and want to ensure that the two tensors share the same data use view else reshape.\n",
    "# Also if you want contiguous memory then use view.\n",
    "\n",
    "tensor_random = torch.randn(4, 4)\n",
    "\n",
    "print(tensor_random)\n",
    "print(tensor_random.shape)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "tensor_random_view_1 = tensor_random.view(16)\n",
    "print(tensor_random_view_1)\n",
    "print(tensor_random_view_1.shape)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "try:\n",
    "  tensor_random_view_2 = tensor_random.view(-1, 7)\n",
    "  print(tensor_random_view_2)\n",
    "  print(tensor_random_view_2.shape)\n",
    "except Exception as e :\n",
    "  print(e)\n",
    "  tensor_random_view_2 = tensor_random.view(-1, 8)\n",
    "  print(tensor_random_view_2)\n",
    "  print(tensor_random_view_2.shape)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "tensor_random = tensor_random * 100\n",
    "print(tensor_random)\n",
    "print(tensor_random_view_1)\n",
    "print(tensor_random_view_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8327,  0.8999, -0.1116, -0.5060,  0.2079,  0.4407,  0.3817,  0.1717,\n",
      "        -0.3775, -1.0016, -1.1113, -0.3706, -0.4343,  0.6793,  2.9176,  0.8866])\n",
      "torch.Size([16])\n",
      "--------------------\n",
      "tensor([[ 0.8327,  0.8999, -0.1116, -0.5060],\n",
      "        [ 0.2079,  0.4407,  0.3817,  0.1717],\n",
      "        [-0.3775, -1.0016, -1.1113, -0.3706],\n",
      "        [-0.4343,  0.6793,  2.9176,  0.8866]])\n",
      "torch.Size([4, 4])\n",
      "--------------------\n",
      "tensor([ 0.8327,  0.8999, -0.1116, -0.5060,  0.2079,  0.4407,  0.3817,  0.1717,\n",
      "        -0.3775, -1.0016, -1.1113, -0.3706, -0.4343,  0.6793,  2.9176,  0.8866])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Reshape - Preffered\n",
    "# Returns a tensor with the same data and number of elements as input, but with the specified shape.\n",
    "\n",
    "tensor_random = torch.randn(16)\n",
    "print(tensor_random)\n",
    "print(tensor_random.shape)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "tensor_random_reshape = torch.reshape(tensor_random, (4, 4))\n",
    "print(tensor_random_reshape)\n",
    "print(tensor_random_reshape.shape)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "tensor_random_flatten =  torch.reshape(tensor_random_reshape, (-1,))\n",
    "print(tensor_random_flatten)\n",
    "print(tensor_random_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1098,  0.2109, -0.1952],\n",
      "        [ 0.3560,  1.2331,  0.0052]])\n",
      "torch.Size([2, 3])\n",
      "--------------------\n",
      "Dimension 0 or default\n",
      "tensor([[[ 0.5174, -1.6801, -1.7602],\n",
      "         [ 1.2056, -0.1794,  0.9064]],\n",
      "\n",
      "        [[ 0.5174, -1.6801, -1.7602],\n",
      "         [ 1.2056, -0.1794,  0.9064]]])\n",
      "torch.Size([2, 2, 3])\n",
      "--------------------\n",
      "Dimension 0\n",
      "tensor([[[ 0.5174, -1.6801, -1.7602],\n",
      "         [ 1.2056, -0.1794,  0.9064]],\n",
      "\n",
      "        [[ 0.5174, -1.6801, -1.7602],\n",
      "         [ 1.2056, -0.1794,  0.9064]]])\n",
      "torch.Size([2, 2, 3])\n",
      "--------------------\n",
      "Dimension 1\n",
      "tensor([[[ 0.5174, -1.6801, -1.7602],\n",
      "         [ 0.5174, -1.6801, -1.7602]],\n",
      "\n",
      "        [[ 1.2056, -0.1794,  0.9064],\n",
      "         [ 1.2056, -0.1794,  0.9064]]])\n",
      "torch.Size([2, 2, 3])\n",
      "--------------------\n",
      "Dimension 2\n",
      "tensor([[[ 0.5174,  0.5174],\n",
      "         [-1.6801, -1.6801],\n",
      "         [-1.7602, -1.7602]],\n",
      "\n",
      "        [[ 1.2056,  1.2056],\n",
      "         [-0.1794, -0.1794],\n",
      "         [ 0.9064,  0.9064]]])\n",
      "torch.Size([2, 3, 2])\n",
      "--------------------\n",
      "Dimension -1\n",
      "tensor([[[ 0.5174,  0.5174],\n",
      "         [-1.6801, -1.6801],\n",
      "         [-1.7602, -1.7602]],\n",
      "\n",
      "        [[ 1.2056,  1.2056],\n",
      "         [-0.1794, -0.1794],\n",
      "         [ 0.9064,  0.9064]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[\\n  [ [0.5174, 0.5174], [-1.6801, -1.6801], [-1.7602, -1.7602] ],   ⬅ Columns stacked element-wise (dim=2)\\n  [ [1.2056, 1.2056], [-0.1794, -0.1794], [ 0.9064,  0.9064] ]\\n]\\n\\nThe new dimension is added after the columns, so each column is paired element-wise from both tensors\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack\n",
    "\n",
    "random_stack = torch.randn(2, 3)\n",
    "print(random_stack)\n",
    "print(random_stack.shape)\n",
    "\n",
    "# OR\n",
    "\n",
    "random_stack = torch.tensor([[ 0.5174, -1.6801, -1.7602],\n",
    "                             [ 1.2056, -0.1794,  0.9064]])\n",
    "\n",
    "'''\n",
    "Shape : 2,3\n",
    "Rows (dim=0): 2\n",
    "Columns (dim=1): 3\n",
    "'''\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "# By Default dimension is 0\n",
    "print(\"Dimension 0 or default\")\n",
    "random_stack_dim_default = torch.stack((random_stack, random_stack))\n",
    "print(random_stack_dim_default)\n",
    "print(random_stack_dim_default.shape)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(\"Dimension 0\")\n",
    "random_stack_dim_0 = torch.stack((random_stack, random_stack), dim=0)\n",
    "print(random_stack_dim_0)\n",
    "print(random_stack_dim_0.shape)\n",
    "\n",
    "'''\n",
    "[        ⬅ New dimension (dim=0, outer)\n",
    "  [ [0.5174, -1.6801, -1.7602],      ⬅ Original tensor 1\n",
    "    [1.2056, -0.1794,  0.9064] ],\n",
    "\n",
    "  [ [0.5174, -1.6801, -1.7602],      ⬅ Original tensor 2\n",
    "    [1.2056, -0.1794,  0.9064] ]\n",
    "]\n",
    "\n",
    "The new dimension is added before the rows\n",
    "'''\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(\"Dimension 1\")\n",
    "random_stack_dim_1 = torch.stack((random_stack, random_stack), dim=1)\n",
    "print(random_stack_dim_1)\n",
    "print(random_stack_dim_1.shape)\n",
    "\n",
    "'''\n",
    "[\n",
    "  [ [0.5174, -1.6801, -1.7602], [0.5174, -1.6801, -1.7602] ],  ⬅ Rows are stacked (dim=1)\n",
    "  [ [1.2056, -0.1794,  0.9064], [1.2056, -0.1794,  0.9064] ]\n",
    "]\n",
    "\n",
    "The new dimension is added between the rows and columns\n",
    "'''\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(\"Dimension 2\") # Results in Tensor Transpose\n",
    "random_stack_dim_2 = torch.stack((random_stack, random_stack), dim=2)\n",
    "print(random_stack_dim_2)\n",
    "print(random_stack_dim_2.shape)\n",
    "\n",
    "'''\n",
    "Shape : 2,3\n",
    "Rows (dim=0): 2\n",
    "Columns (dim=1): 3\n",
    "\n",
    "So we are going to insert new dimesnion after column\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(\"Dimension -1\") # Results in Tensor Transpose\n",
    "random_stack_dim_neg_1 = torch.stack((random_stack, random_stack), dim=-1)\n",
    "print(random_stack_dim_neg_1)\n",
    "print(random_stack_dim_neg_1.shape)\n",
    "\n",
    "'''\n",
    "[\n",
    "  [ [0.5174, 0.5174], [-1.6801, -1.6801], [-1.7602, -1.7602] ],   ⬅ Columns stacked element-wise (dim=2)\n",
    "  [ [1.2056, 1.2056], [-0.1794, -0.1794], [ 0.9064,  0.9064] ]\n",
    "]\n",
    "\n",
    "The new dimension is added after the columns, so each column is paired element-wise from both tensors\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3cXYT_oK436"
   },
   "source": [
    "# Building a Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jXMd8PPhFWxv"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QjwZKCwxIbCL"
   },
   "outputs": [],
   "source": [
    "# Input data (features)\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "\n",
    "# Output data (targets)\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Qa83_aHSLQ1S"
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # Define the model's parameters\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gskfXvfSLZxL"
   },
   "source": [
    "* class LinearRegressionModel(nn.Module): We create a class that inherits from nn.Module, which is essential for any PyTorch model.\n",
    "* def __init__(self): The constructor where we define the layers and parameters of our model.\n",
    "* super().__init__(): Initializes the base class nn.Module.\n",
    "* self.linear = nn.Linear(in_features=1, out_features=1): A linear layer that applies a linear transformation\n",
    "𝑦 = 𝑥 ⋅ weight + bias.\n",
    "* in_features=1: The number of input features (since each input x is a single value).\n",
    "* out_features=1: The number of output features (predicting a single value).\n",
    "* def forward(self, x): The forward pass method where computation happens.\n",
    "* out = self.linear(x): Applies the linear transformation to the input.\n",
    "* return out: Returns the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d7duSNIJbRk"
   },
   "source": [
    "# Key PyTorch Components\n",
    "## torch.nn\n",
    "Contains all the building blocks for neural networks in PyTorch, such as layers, activation functions, and loss functions. It allows you to construct computational graphs, which are sequences of computations executed in a specific order.\n",
    "\n",
    "## torch.nn.Parameter\n",
    "A subclass of torch.Tensor that is automatically registered as a parameter when assigned as an attribute to an nn.Module. Parameters are tensors that are considered model parameters, and if requires_grad=True (the default), gradients are automatically computed for them during backpropagation.\n",
    "\n",
    "## torch.nn.Module\n",
    "The base class for all neural network modules in PyTorch. It provides a way to encapsulate parameters, helpers for moving them to GPUs, exporting, loading, and more. All custom models should subclass nn.Module and implement the forward() method.\n",
    "\n",
    "## torch.optim\n",
    "Contains optimization algorithms (optimizers) used to update the parameters of a model based on the gradients computed during backpropagation. Optimizers adjust the weights to minimize the loss function.\n",
    "\n",
    "## def forward()\n",
    "All subclasses of nn.Module must implement the forward() method, which defines how the model processes input data and produces output. This is where the actual computation happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x9KHtYxPLT-I"
   },
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PS06byeBMc-x"
   },
   "source": [
    "* model: An instance of our linear regression model.\n",
    "* The model's parameters (weights and biases) are automatically registered as nn.Parameter objects because they're defined within an nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZEk_pdIWMN1Q"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpWeRoYfMT3o"
   },
   "source": [
    "* criterion = nn.MSELoss(): The Mean Squared Error loss function, which measures the average squared difference between the predicted and actual values.\n",
    "* optimizer = torch.optim.SGD(model.parameters(), lr=0.01):\n",
    "* torch.optim.SGD: The Stochastic Gradient Descent optimizer.\n",
    "* model.parameters(): Retrieves the model's parameters that need to be updated.\n",
    "* lr=0.01: The learning rate, controlling how much to adjust the parameters during each update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1726714499681,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "IrpOhem6Mgs3",
    "outputId": "8b431b07-bac5-48ca-afdd-0eda2ea6ccc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Loss: 40.9930\n",
      "Epoch [4/4], Loss: 19.8005\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: Compute predicted y\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights tensor([[0.4969]])\n",
      "Model bias tensor([0.4484])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model weights {model.linear.weight.data}\")\n",
    "print(f\"Model bias {model.linear.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB6OLiDAM0ox"
   },
   "source": [
    "* Training Loop: Iterates over the dataset multiple times (epochs) to train the model.\n",
    "* y_pred = model(X): Performs the forward pass by calling the model on the input data. This invokes the forward() method.\n",
    "* loss = criterion(y_pred, y): Computes the loss between the predicted values and actual values.\n",
    "* optimizer.zero_grad(): Clears old gradients before computing new ones.\n",
    "* loss.backward(): Performs backpropagation to compute gradients of the loss w.r.t. model parameters.\n",
    "* optimizer.step(): Updates the model parameters using the computed gradients.\n",
    "* Progress Printing: Every 2 epochs, prints the current loss to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1726714968115,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "vtsKqHLeLFFw",
    "outputId": "edaf674c-3757-4bdd-eeee-2b0ca2c6d1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4])\n",
      "====================\n",
      "Input values:  tensor([1., 2., 3., 4.])\n",
      "Input values:  [1. 2. 3. 4.]\n",
      "====================\n",
      "Predicted values: [0.94528663 1.4421527  1.9390187  2.4358847 ]\n",
      "====================\n",
      "Actual values: [2. 4. 6. 8.]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():  # Disables gradient calculation\n",
    "    predicted = model(X)\n",
    "    print(X.ndim)\n",
    "    print(X.shape)\n",
    "    print(X.squeeze().shape)\n",
    "    print('='*20)\n",
    "    print('Input values: ', X[:,0])\n",
    "    print('Input values: ', X.squeeze().numpy())\n",
    "    print('='*20)\n",
    "    print('Predicted values:', predicted.squeeze().numpy())\n",
    "    print('='*20)\n",
    "    print('Actual values:', y.squeeze().numpy())\n",
    "\n",
    "# torch.squeeze removes dimensions of size 1 from a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "86nh53E7NP4i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3]]])\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[3]]])\n",
    "print(a)\n",
    "a_new = a.squeeze()\n",
    "print(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLfg4nlC8GQ3PoRG9Otk06",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cv_live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
