{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()  # Initialize the parent class (nn.Module)\n",
    "\n",
    "        # First convolutional layer: 1 input channel (grayscale), 6 output channels, 5x5 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n",
    "\n",
    "        # First average pooling layer: 2x2 kernel, stride 2\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second convolutional layer: 6 input channels, 16 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "\n",
    "        # Second average pooling layer: 2x2 kernel, stride 2\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # First fully connected layer: 16*5*5 input features, 120 output features\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "\n",
    "        # Second fully connected layer: 120 input features, 84 output features\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "\n",
    "        # Third fully connected layer: 84 input features, 10 output features (for 10 classes)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        # Activation function: Tanh (used after convolutional and fully connected layers)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolutional layer followed by Tanh activation\n",
    "        x = self.activation(self.conv1(x))\n",
    "\n",
    "        # Apply first average pooling layer\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Apply second convolutional layer followed by Tanh activation\n",
    "        x = self.activation(self.conv2(x))\n",
    "\n",
    "        # Apply second average pooling layer\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(-1, 16 * 5 * 5)  # Reshape to [batch_size, 16*5*5]\n",
    "\n",
    "        # Apply first fully connected layer followed by Tanh activation\n",
    "        x = self.activation(self.fc1(x))\n",
    "\n",
    "        # Apply second fully connected layer followed by Tanh activation\n",
    "        x = self.activation(self.fc2(x))\n",
    "\n",
    "        # Apply third fully connected layer (no activation, outputs raw logits)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = LeNet5()  # Instantiate the LeNet-5 model\n",
    "\n",
    "# Create a random input tensor: batch of 1, 1 channel (grayscale), 32x32 image\n",
    "input_tensor = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "# Pass the input through the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Print the output shape: should be [1, 10] (batch of 1, 10 classes)\n",
    "print(output.shape)  # Output shape: [1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 0.3040\n",
      "Epoch [1/10], Step [200/938], Loss: 0.2576\n",
      "Epoch [1/10], Step [300/938], Loss: 0.1408\n",
      "Epoch [1/10], Step [400/938], Loss: 0.1550\n",
      "Epoch [1/10], Step [500/938], Loss: 0.0666\n",
      "Epoch [1/10], Step [600/938], Loss: 0.2834\n",
      "Epoch [1/10], Step [700/938], Loss: 0.1280\n",
      "Epoch [1/10], Step [800/938], Loss: 0.1558\n",
      "Epoch [1/10], Step [900/938], Loss: 0.1157\n",
      "Epoch [1/10], Average Loss: 0.2677\n",
      "Epoch [2/10], Step [100/938], Loss: 0.1669\n",
      "Epoch [2/10], Step [200/938], Loss: 0.0360\n",
      "Epoch [2/10], Step [300/938], Loss: 0.0463\n",
      "Epoch [2/10], Step [400/938], Loss: 0.0543\n",
      "Epoch [2/10], Step [500/938], Loss: 0.1439\n",
      "Epoch [2/10], Step [600/938], Loss: 0.0180\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0662\n",
      "Epoch [2/10], Step [800/938], Loss: 0.2303\n",
      "Epoch [2/10], Step [900/938], Loss: 0.1279\n",
      "Epoch [2/10], Average Loss: 0.0861\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0855\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0268\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0912\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0104\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0960\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0419\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0294\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0616\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0239\n",
      "Epoch [3/10], Average Loss: 0.0594\n",
      "Epoch [4/10], Step [100/938], Loss: 0.1286\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0039\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0256\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0131\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0085\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0429\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0055\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0452\n",
      "Epoch [4/10], Average Loss: 0.0449\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0283\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0370\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0400\n",
      "Epoch [5/10], Step [400/938], Loss: 0.2006\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0227\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0398\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0556\n",
      "Epoch [5/10], Average Loss: 0.0359\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0177\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0173\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0201\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0034\n",
      "Epoch [6/10], Step [500/938], Loss: 0.1045\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0079\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0482\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0108\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0099\n",
      "Epoch [6/10], Average Loss: 0.0300\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0038\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0142\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0038\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0223\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0147\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0355\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0719\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0104\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0072\n",
      "Epoch [7/10], Average Loss: 0.0248\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0072\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0193\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0334\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0043\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0106\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0050\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0245\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0129\n",
      "Epoch [8/10], Average Loss: 0.0221\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0032\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0211\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0525\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0171\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0038\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0055\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0041\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0030\n",
      "Epoch [9/10], Average Loss: 0.0182\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0116\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0501\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0023\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0396\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0056\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0099\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0089\n",
      "Epoch [10/10], Average Loss: 0.0163\n",
      "Test Accuracy: 98.60%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the LeNet-5 model (same as before)\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 16 * 5 * 5)  # Flatten\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Transformations for the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LeNet5()\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for data, target in test_loader:\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get a random image and label from the test dataset\n",
    "data, target = next(iter(test_loader))  # Get a batch of data\n",
    "image, label = data[13], target[13]  # Pick the first image and label from the batch\n",
    "\n",
    "# Add a batch dimension (since the model expects a batch of images)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "# Forward pass to get the predicted label\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)  # Get the predicted class\n",
    "\n",
    "# Convert the image tensor to a numpy array for visualization\n",
    "image = image.squeeze().numpy()  # Remove batch dimension and convert to numpy\n",
    "\n",
    "# Display the image and labels\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f\"Ground Truth: {label}, Predicted: {predicted.item()}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
